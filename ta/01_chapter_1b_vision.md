
**பாடம் I: ஆழக்கற்றல் தோற்றம் (தொடர்கிறது)**

**உயிரியல் பார்வை**

**இயந்திரப் பார்வை**

உயிர்களின் ஒளி உணர் திறன் பற்றி நாம் விரிவாக விவாதித்ததற்கு காரணம் என்ன என்பது இப்போது தெளிவாகி இருக்கும் என்று எண்ணுகிறேன். இன்றைய ஆழக்கற்றலுக்கும் உயிர்களின் ஒளி உணர் திறனுக்கும் உள்ள தொடர்பை நான் இப்போது நான் நன்றாத தெரிந்து கொண்டோம்.


*இந்த படத்தில் உயிர்களின் ஒளி உணரும் திறனில் ஏற்பட்ட மாற்றங்களையும் இயந்திரக்கற்றல் வளர்சியையும் காட்டுகிறது.  நீல நிறக்கோடு ட்ரைலோபைட்டுகளில் பார்வை வளர்ச்சியையும், ஹூபல் மற்றும் வீசலின் 1959 இல் முதன்மைக் கார்ட்டெக்ஸ் ஆய்வையும் காட்டுகிறது. இயந்திரப் பார்வை(machine vision) கால வரிசை பிங்க், மற்றும் ஊதா நிற கோடுகளாகப் பிரிக்கப்பட்டுள்ளது. பிங்க் நிற கோடு ஆழக்கற்றல் வளர்சியையும் ஊதா நிற கோடு இயந்திரக்கற்றல் வளர்சியையும் காட்டுகிறது.*


**நியோகாக்னிட்ரானை அறிவோம்**

முதன்மை காட்சி புறணி படிநிலையை(hierarchy) உருவாக்கும் எளிய மற்றும் சிக்கலான செல்களை ஹூபல் மற்றும் வைசல் கண்டுபிடித்தனர்.   ஹூபல் மற்றும் வைசல் கண்டுபிடிப்பால்   ஈர்க்கப்பட்டு, 1970 களின் பிற்பகுதியில், ஜப்பானிய மின் பொறியாளர் குனிஹிகோ புகுஷிமா இயந்திர பார்வைக்கு ஒரு ஒத்த கட்டமைப்பை முன்மொழிந்தார். அதற்கு அவர் நியோகாக்னிட்ரான் என்று பெயரிட்டார்.  புகுஷிமா தனது எழுத்தில் ஹூபல் மற்றும் வீசலின்  முதன்மைக் காட்சிப் புறணி அமைப்பைப் பற்றிய அவர்களின் மூன்று முக்கிய கட்டுரைகளை மேற்கோள்காட்டி விளக்கினார்.



*குனிஹிகோ புகுஷிமா(Kunihiko Fukushima)*

*Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cynbernetics, 36, 193– 202.* 

*படிநிலை முறையில் செயற்கை நியூரான்களை ஒழுங்குபடுத்துவதன் மூலம், செயற்கை நியூரான்கள் - உயிர்ப்புள்ள உண்மை நியூரான்கள் போலவே, - *
*By arranging artificial neurons in this hierarchical manner, these artificial neurons— like their biological inspiration earlier — generally represent line orientations in the cells of the layers closest to the raw visual image, while successively deeper layers represent successively complex, successively abstract objects.*



**LeNet-5**
நியோ காக்நெட்ரானை கையெழுத்த


நியோகாக்னிட்ரான் கையால் எழுதப்பட்ட எழுத்துக்களை அடையாளம் காணும் திறன் கொண்டதாக இருந்தாலும்,யான் லீகூன் மற்றும் யோஷுவா பென்ஜியோ இருவரும் இணைந்து இன்னும் துள்ளியமான மற்றும் அதிக செயல் திறன் கொண்ட LeNet-5 ஐ உருவாக்கினார்கள். புகுஷிமாவின் தலைமையில்  ஹூபல் மற்றும் வைசலின் ஆராய்சியை மையமாகக் கொண்டு, லெனெட் -5 உருவாக்கப்பட்டது.  யான் லீகூன் மற்றும் யோஷுவா பென்ஜியோ தரமான தரவுகள், அதிக திறன் கொண்ட கணினிகள் மற்றும் பின்னோக்கு பரவல் அல்காரிதம்களைக் கொண்டு அவர்கள் லெனெட் -5 ஐ உருவாக்கினார்கள்.


செயற்கை நரம்பியல் வலையமைப்பு மற்றும் ஆழக்கற்றல் ஆராய்ச்சியில்  பாரிஸில் பான் - யான் மற்றும் லீகன் இருவரும் மிக முக்கியமானவர்கள். லீகன் நியூயார்க் பல்கலைக்கழக அறிவியல் மையத்தின் நிறுவன இயக்குநராகவும், பேஸ்புக் நிறுவனத்தில் AI ஆராய்ச்சியின் இயக்குநராகவும் உள்ளார்.


யோஷுவா பெங்கியோ செயற்கை நரம்பியல் நெட்வொர்க்குகள் மற்றும் ஆழ்ந்த கற்றல் துறையில் மிகவும் முக்கியம் வாந்த விஞ்ஞானி. பிரான்சில் பிறந்த இவர், மாண்ட்ரீல் பல்கலைக்கழகத்தில் கணினி அறிவியல் பேராசிரியராக உள்ளார், மேலும் கனடிய இன்ஸ்டிடியூட் ஃபார் அட்வான்ஸ்ட் ரிசர்ச்சில் புகழ்பெற்ற இயந்திரங்கள் மற்றும் மூளை திட்டத்தின் இணை இயக்குனராக உள்ளார்.



மேலே உள்ள இரண்டு வரைபடங்கள் ஒரே லெனெட் -5 கட்டமைப்பின் இரண்டு விதமாக காட்டப்பட்டுள்ளது, முதலாவது ஒரு தர்க்கரீதியான பார்வை மற்றும் இரண்டாவது ஒரு “தொழில்நுட்ப” பார்வை. ஹூபல் மற்றும் வீசல் ஆகியோரால் கண்டுபிடிக்கப்பட்ட முதன்மை காட்சி கோர்டெக்ஸின் படிநிலை கட்டமைப்பை லெனெட் -5 எவ்வாறு தக்க வைத்துக் கொள்கிறது என்பதையும்பு குஷிமா தனது நியோகாக்னிட்ரானில் எவ்வாறு பயன்படுத்தியுள்ளார் என்பதையும் காணலாம். படத்தில் இடதுபுற அடுக்கு எளிய விளிம்புகளையும், அடுத்தடுத்த அடுக்குகள் பெருகிய முறையில் சிக்கலான அம்சங்களையும் குறிக்கின்றன. இந்த வழியில் தகவல்களைச் செயலாக்குவதன் மூலம், கையால் எழுதப்பட்ட “2”, எடுத்துக்காட்டாக, எண் இரண்டாக சரியாக கண்டறியும்.

மேலே உள்ள படத்தில் லெனெட்-5 எவ்வாறு செயல்படுகிறது என்று பார்க்கலாம். அது எண் இரண்டை (2) உள்வாங்கி அடுத்த அடுக்குக்கு அனுப்புகிறது. இந்த அடுக்கில் உள்ளிடப்பட்ட தகவலின் அம்சங்கள்(features) பெரிய அம்சங்களாக பிரிகிறது. அடுத்த அடுக்கில் சிறு சிறு அம்சங்களாக பிரிகிறது. அடுத்த அடுக்கில் சிறு கன சதுரங்களாக காட்டப்பட்டுள்ளது. அடுத்தடுத்த அடுக்குகளில் இந்த கன சதுரங்கள் தடிமன் குறைந்து நீளம் அதிகரித்து காணப்படும். இறுதியாக கோடுகள் போல காட்சியளிக்கும்.  இறுதி அடுக்கு சரியான எண்ணை விடையாக வெளியிடும்.

*Fukushima, K., & Wake, N. (1991). Handwritten alphanumeric character recognition by the neocognitron. IEEE Transactions on Neural Networks, 2, 355– 65.*

*LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 2, 355– 65. 11.*

லெனெட்-5 என்ற மாற்றத்த நரம்பியல் வலைப்பின்னல் ஒரு ஆழக்கற்றல் வகையைச் சேர்ந்த அல்காரிதம் ஆகும். இந்த வலையமைப்பு பிற்காலத்தில் இயந்திரப்பார்வை எனப்படும் மிஷன் விஷன் திட்டத்தில் முக்கிய பங்கு வகிக்க்கவிருக்கிறது. ெனெட்-5இல் பயன்படுத்திய  MNIST எனப்படும் கையாள் எழுதப்பட்ட எண்களின் மாதிரி தரவுகளை  புத்தகத்தின் பகுதி -2 இல் "அத்தியாவசிய கோட்பாடுகள் மற்ற்றும் விளக்கங்கள்" என்ற பகுதியில் அதிகம் பயன்படுத்தவிருக்கிறோம். ஆழக்கற்றல் மாதிரிகளில் அதிகம் பயன்படுத்தப்படும் பின்னோக்கு பரவல் முறையை பின் வரும் பகுதிகளில் தெரிந்து கொள்ளலாம்.

இதனைத் தொடர்ந்து அமெரிக்க தபால் சேவை நிறுவனம் பின்கோடுகளைக் கண்டுபிடிக்க லெனெட்-5ஐ வணிகரீதியாக பயன்படுத்தத் தொடங்கியது. எனவே லெனெட் 5 மூலம் சாதாரண இயந்திரக் கற்றலுக்கும் ஆழக்கற்றலுக்கும் உள்ள வேறுபாட்டை தெளிவாக தெரிந்துகொள்ளலாம்.

படம் 1.12 இல் உள்ளது போல் இயந்திரக்கற்றல் முறை முக்கிய அம்சங்களைக் கற்றல் முறையைப் பயன்படுத்துகிறது. ஒரு தரவை எடுத்து அதை தூய்மையாக்கி, முக்கிய அம்சங்களைத் தேர்ந்தெடுத்து புள்ளி விவரங்களின் அடிப்படையில் மாதிரிகளை உருவாக்கலாம்.  நிக் ரிக்ரஷன், ரேண்டம் ஃப்பாரஸ்ட் மற்ற்றும் வெக்டார் இய்ந்திரம் போன்ற முறைகள்  பதப்படுத்தப்படாத தரவுகளைக் கொண்டு சிறப்பான முடிவுகளைத் தருகிறது.


இயந்திரக் கற்றலில் அம்சங்களை தேர்ந்ந்நதெடுத்தல் என்பது ஒரு அறிவியல் மற்றும் கலை எனலாம். இதற்கு துறையில் நிபுணத்துவமும், உள்ளுணர்வும், கணிதத்திறமையும் மிகவும் முக்கியம்.  

Feature Engineering, quite simply, is the “science/art” of representing data in the best way possible for ML algorithms to be effective. Why “science/art”? Because good feature engineering involves an elegant blend of domain knowledge, intuition, and basic mathematical abilities. Actually, a lot of the most effective data representation “hacks” barely involve any mathematical computation at all! What does “best” mean? In essence, the data is presented to the algorithm should denote the pertinent structures/properties of the underlying information in the most effective way possible. When you do feature engineering, you are essentially converting your data attributes into data features. Attributes are basically all the dimensions (columns) present in your data. But do all of them, in the raw format, represent the underlying trends you want to learn in the best way possible? Maybe not. So, what you do in feature engineering, is pre-process your data so that your model/learning algorithm has to spend minimum effort on wading through noise. What I mean by ‘noise’ here, is any information that is not relevant to learning/predicting your ultimate goal. In fact, using good features can even let you use considerably simpler models since you are doing a part of the thinking yourself.

Feature engineering— the transformation of raw data into thoughtfully transformed input variables— often predominates the application of traditional machine learning algorithms. In contrast, the application of deep learning often involves little to no feature engineering. The majority of time is spent instead on the design and tuning of the deep learning model architecture. In contrast, a minority of the traditional ML practitioner’s time is spent optimizing ML models or selecting the most effective one from those available.

The deep learning approach to modeling data turns these traditional ML priorities upside down. The deep learning practitioner typically spends little to none of her time engineering features, instead spending it modeling data with various artificial neural network architectures that process the raw inputs directly into useful features automatically. This distinction between deep learning and traditional machine learning is a core theme of this book.

But for a long time, traditional machine learning, despite its feature engineering issues, was considered to hold more promise, and funding shifted away from deep learning research. This was understandable as, at the time, there were stumbling blocks associated with optimizing deep learning models. This included poor weight initializations (we’ll cover this in later chapters), covariate shift (also in later chapters), and the predominance of the relatively inefficient sigmoid activation function (later chapters). Public funding for artificial neural network research ebbed globally, with the notable exception of continued support from the Canadian federal government, enabling the Universities of Montreal, Toronto, and Alberta to subsequently become powerhouses in the field.

Here you see a celebrated example of successful feature engineering from Paul Viola and Michael Jones in the early 2000s. Viola and Jones employed rectangular filters such as the vertical or horizontal black-and-white bars shown in the figure. These features generated by passing rectangular filters over an image were fed into machine learning algorithms to reliably detect the presence of a face. Their efficient algorithm found its way into Fujifilm cameras, facilitating real-time auto-focus. In the figure, the combinations of rectangular filters and an original image used for the purpose of reliable face recognition are shown. The first filter is a horizontal white bar inside a horizontal black rectangular box. The second filter is a white vertical bar inside a black rectangular box. The original image is a thumbnail of a person's face. Both filters cover the eye region of the person's face. This work is notable because the algorithm was efficient enough to be the first real-time face detector outside the realm of biology.

Devising clever face-detecting filters to process raw pixels into features for input into a machine learning model was accomplished via years of research and collaboration on the characteristics of faces. And, of course, it is limited to detecting faces in general, as opposed to being able to recognize a particular face as, say, your own face or your favorite person’s face. To develop features for detecting your face in particular, or for detecting some non-face class of objects like houses, cars, or cricket balls, would require the development of expertise in that category, something that could again take years of academic-community collaboration to execute both efficiently and accurately. The machine vision community was looking for a way out of this tricky problem.

ImageNet and the ILSVRC

The success LeNet-5 had over the neocognitron was catalyzed by the availability of a larger, high-quality set of training data. There was a clear recognition among the scientific community that data driven discoveries were cropping up many research areas. The next breakthrough in neural networks was also catalyzed by a high-quality public dataset, this time much larger. ImageNet, a labeled index of photographs devised by computer science professor Fei-Fei Li armed machine vision researchers with an immense catalog of training data. For reference, the handwritten digit data used to train LeNet-5 contained tens of thousands of images. ImageNet, in contrast, contains tens of millions. The 14 million images in the ImageNet dataset are spread across 22,000 categories. These categories are as diverse as container ships, leopards, starfish, and elderberries.

Fei-Fei Li

The massive ImageNet dataset was the brainchild of a computer science professor Fei-Fei Li and her colleagues at Princeton in 2009. Now a faculty member at Stanford University, Li is also the chief scientist of A.I./ ML for Google’s cloud platform. In 2010, Li began running an open challenge called ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) on a subset of the ImageNet data. This challenge has, over time, become the premier ground for assessing the world’s state-of-the-art machine vision algorithms. The ILSVRC subset consists of 1.4 million images across 1,000 categories. In addition to providing a broad range of categories, many of the selected categories are breeds of dogs, thereby evaluating the algorithms’ ability not only to distinguish widely varying images but also to specialize in distinguishing subtly varying ones.

Aside: Just as a thought experiment, try to distinguish photos of Rampur Greyhound from Mudhol Hound. It’s tough, but dog experts, as well as deep learning machine vision models, can do it!

Rampur Greyhound vs Mudhol Hound

Tangentially, these dog-heavy data are the reason deep learning models trained with ImageNet have a disposition toward “dreaming” about dogs (see, e.g., deepdreamgenerator.com).

The arrival of the gamechanger a.k.a. AlexNet

The graph above shows that in the first two years of the ILSVRC all algorithms entered into the competition hailed from the feature-engineering-driven traditional machine learning ideology. In the third year, all entrants except one were traditional ML algorithms. If that one deep learning model in 2012 had not been developed or if its creators had not competed in ILSVRC, then the year-over-year image classification accuracy would have been negligible. Instead, Alex Krizhevsky and Ilya Sutskever— working out of the University of Toronto lab led by Geoffrey Hinton— crushed all existing benchmarks with their submission, today referred to as AlexNet.

AlexNet was the victor by a head-and-shoulders (40 percent!) margin in the 2012 iteration. This was a watershed moment for Deep Learning! Instantly, deep learning architectures emerged from the fringes of machine learning to its fore. Academics and commercial practitioners scrambled to grasp the fundamentals of artificial neural networks as well as to create software libraries— many of them open-source— to experiment with deep learning models on their own data and use cases, be they machine vision or otherwise. Again, as the ILSRVC results graph illustrates, in the years since 2012 all of the top-performing models in the ILSVRC have been based on deep learning. All of the best algorithms since then have been deep learning models.

Things did not stop there. In 2015, machines surpassed human accuracy. In the graph, the horizontal axis represents years and ranges from 2010 to 2017 in increments of 1. The vertical axis represents the top 5 error rate and ranges from 0 to 0.6 in increments of 0.2. A box plot is shown for the year 2010 in which the minimum error value falls at 0.26 and maximum value at 0.45. In the year 2011, the box plot shows a maximum value of 0.5 and a minimum value of 0.24. In the year 2012, the traditional machine learning shows a value of 0.23 but AlexNet showed a lower error rate of 0.18. The plots shown until 2012 is traditional machine learning but after the introduction of the AlexNet, the successive years shows the performance of deep learning. The error rates have a decreasing trend until the year 2017.

Geoffrey Hinton the eminent British-Canadian artificial neural network pioneer, habitually referred to as “the godfather of deep learning” in the popular press. He is an emeritus professor at the University of Toronto and an engineering fellow at Google, responsible for managing the search giant’s Brain Team, a research arm, in Toronto. In 2019, Hinton, Yann LeCun, and Yoshua Bengio were jointly recognized with the Turing Award— the highest honor in computer science— for their work on deep learning.

You can see in this figure that AlexNet’s hierarchical architecture is reminiscent of LeNet-5, with the first (left-hand) layer representing simple visual features like edges, and deeper layers representing increasingly complex features and abstract concepts. Shown at the bottom are examples of images to which the neurons in that layer maximally respond, recalling the layers of the biological visual system and demonstrating the hierarchical increase in visual feature complexity. In the example shown here, an image of a cat input into LeNet-5 is correctly identified as such (as implied by the green “CAT” output). “CONV” indicates the use of something called a convolutional layer, and “FC” is a fully connected layer (don’t worry we will formally explain these layer types in later chapters).

Though the architecture of AlexNet is reminiscent of LeNet-5, there are three principal factors that enabled AlexNet to be the state-of-the-art machine vision algorithm in 2012:

Training data: Not only did Krizhevsky and his colleagues have access to the massive ImageNet index, they also artificially expanded the data available to them by applying transformations to the training images (you, too, will do this later).

Processing power: Not only had computing power per unit of cost increased dramatically from 1998 to 2012, but Krizhevsky, Hinton, and Sutskever also programmed two GPUs24 to train their large datasets with previously unseen efficiency.

Architectural Advances: AlexNet is deeper (has more layers) than LeNet-5, and it takes advantage of both a new type of artificial neuron and a nifty trick that helps generalize deep learning models beyond the data they’re trained on.

GPU stands for "Graphics Processing Unit." A GPU is a processor designed to handle graphics operations. This includes both 2D and 3D calculations, though GPUs primarily excel at rendering 3D graphics. Early PCs did not include GPUs, which meant the CPU had to handle all standard calculations and graphics operations. As software demands increased and graphics became more important (especially in video games), a need arose for a separate processor to render graphics. On August 31, 1999, NVIDIA introduced the first commercially available GPU for a desktop computer, called the GeForce 256.

It could process 10 million polygons per second, allowing it to offload a significant amount of graphics processing from the CPU. The success of the first graphics processing unit caused both hardware and software developers alike to quickly adopt GPU support. Motherboards were manufactured with faster PCI slots and AGP slots, designed exclusively for graphics cards, became a common option as well. Software APIs like OpenGL and Direct3D were created to help developers make use of GPUs in their programs. Today, dedicated graphics processing is standard – not just in desktop PCs – but also in laptops, smartphones, and video game consoles. These are designed primarily for rendering video games but are well suited to performing the matrix multiplication that abounds in deep learning across hundreds of parallel computing threads.

What really makes the industry practitioners view Deep Learning as “Disruptive”?

The ILSVRC case study underlines why deep learning models like AlexNet are considered widely applicable and disruptive across industries and computational applications. These models dramatically reduce the subject-matter expertise required for building highly accurate predictive models. This trend away from expertise-driven feature engineering and toward surprisingly powerful automatic-feature-generating deep learning models has been prevalently borne out across not only vision applications, but also, for example, the playing of complex games (the topic of Chapter 4) and natural language processing (Chapter 2).

Today, one no longer needs to be a specialist in the visual attributes of faces to create a face-recognition algorithm. One no longer requires a thorough understanding of a game’s strategies to write a program that can master it. One no longer needs to be an authority on the structure and semantics of each of several languages to develop a language-translation tool. For a rapidly growing list of use cases, one’s ability to apply deep learning techniques outweighs the value of domain-specific proficiency. While such proficiency formerly may have necessitated a doctoral degree or perhaps years of postdoctoral research within a given domain, a functional level of deep learning capability can be developed with relative ease— as by working through this book!

TensorFlow Playground

For a fun, interactive way to crystallize the hierarchical, feature-learning nature of deep learning, make your way to the TensorFlow Playground at bit.ly/ TFplayground. When you use this custom link, your network should automatically look similar to the one shown in the figure below.

In Part II we will return to define all of the terms on the screen; for the present exercise, they can be safely ignored. It suffices at this time to know that this is a deep learning model. The model architecture consists of six layers of artificial neurons: an input layer on the left (below the “FEATURES” heading), four “HIDDEN LAYERS” (which bear the responsibility of learning), and an “OUTPUT” layer (the grid on the far right ranging from –6 to + 6 on both axes).

The network’s goal is to learn how to distinguish orange dots (negative cases) from blue dots (positive cases) based solely on their location on the grid. As such, in the input layer, we are only feeding in two pieces of information about each dot: its horizontal position (X1) and its vertical position (X2). The dots that will be used as training data are shown by default on the grid. By clicking the Show test data toggle, you can also see the location of dots that will be used to assess the performance of the network as it learns. Critically, these test data are not available to the network while it’s learning, so they help us ensure that the network generalizes well to new, unseen data.

This deep neural network is ready to learn how to distinguish a spiral of orange dots (negative cases) from blue dots (positive cases) based on their position on the X1 and X2 axes of the grid on the right. "A screenshot shows the Tensorflow interface. The interface is divided into four parts, vertically: data, features, hidden layers, and output. Data: four types of dataset are given, of which the spiral type is selected. The ratio of training to test data is set to 50 percent, the noise is set to 0 and the batch size is set to 10. The section has "Regenerate" command button. The next one is "Features" which includes options to select several input layers. Here, two inputs X1 and X2 are selected. To the right of the 'features' is the hidden layers section, with 4 layers placed one next to another. There are add and delete buttons to increase or reduce the number of neurons in each layer. The 4 layers are interconnected. Here, the first layer has 8 neurons, the second layer has 8 neurons, the third layer has 4 neurons and the fourth layer has 2 neurons. There are add and delete buttons to modify the number of hidden layers too. The final hidden layer is connected to the output.

The output here shows a spiral-shaped graph constituted of orange and blue dots lying alternatively in the graph. In the output, test loss is 0.501 and training loss is 0.509. Below the output is a scale where "colors shows data, neurons and weight values." For pure orange it starts from -1, for white, it is 0, and for blue it is 1. Below this are two checkboxes titled "Show test data" and "Discretize output," both not selected. Click the prominent Play arrow in the top-left corner. Enable the network to train until the “Training loss” and “Test loss” in the top-right corner have both approached zero— say, less than 0.05. How long this takes will depend on the hardware you’re using but hopefully will not be more than a few minutes.

As captured in the figure below, you should now see the network’s artificial neurons representing the input data, with increasing complexity and abstraction the deeper (further to the right) they are positioned— as in the neocognitron, LeNet-5, and AlexNet. Every time the network is run, the neuron-level details of how the network solves the spiral classification problem are unique, but the general approach remains the same (to see this for yourself, you can refresh the page and retrain the network).

The artificial neurons in the leftmost hidden layer are specialized in distinguishing edges (straight lines), each at a particular orientation. Neurons from the first hidden layer pass information to neurons in the second hidden layer, each of which recombines the edges into slightly more complex features like curves. The neurons in each successive layer recombine information from the neurons of the preceding layer, gradually increasing the complexity and abstraction of the features the neurons can represent. By the final (rightmost) layer, the neurons are adept at representing the intricacies of the spiral shape, enabling the network to accurately predict whether a dot is orange (a negative case) or blue (a positive case) based on its position (its X1 and X2 coordinates) in the grid. Hover over a neuron to project it onto the far-right “OUTPUT” grid and examine its individual specialization in detail.

Figure 1.19 The network after training The TensorFlow screen shows the output of the trained network. The spiral dataset is selected. The input layer has values X1 and X2 fed into it. There are 4 hidden layers, that have 8 neurons, 8 neurons, 4 neurons, and 2 neurons respectively. The number of hidden layers and the number of neurons in a layer are editable. The output shows a test loss of 0.010 and a training loss of 0.004. The final output shows the blue and orange spiral surrounded by blue or orange region. The blue spiral lies within the blue region and the orange spiral lies within the orange region. This shows that the trained network correctly detected the blue and orange dots with the X1 and X2 coordinates.

Realtime doodling and machine vision

To interactively experience a deep learning network carrying out a machine vision task in real time, navigate to quickdraw.withgoogle.com to play the Quick, Draw! game.

Click Let’s Draw! to begin playing the game. You will be prompted to draw an object, and a deep learning algorithm will guess what you sketch. By the end of Chapter 10, we will have covered all of the theory and practical code examples needed to devise a machine vision algorithm similar to this one! To boot, the drawings you create will be added to the dataset that you’ll leverage in Chapter 12 when you create a deep learning model that can convincingly mimic human-drawn doodles. So, come with us on this fantastic journey!

Summary In this chapter, we explored the history of deep learning from its evolutionary origins through to the AlexNet watershed moment in 2012 that brought the technique to the forefront of scientific attention. Through all of this, the point to keep in mind is that it is the hierarchical architecture of deep learning models that enables them to encode increasingly complex representations. We explored this concept with an interactive demonstration of hierarchical representations in action by training an artificial neural network in the TensorFlow Playground. We then looked at a system that recognizes doodles “automagically” using deep learning!

In the next chapter, we will expand on the ideas introduced in this chapter by moving from vision applications to language applications!
