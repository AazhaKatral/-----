**எளிய முறையில் ஆழக்கற்றல் **

**ஆழக்கற்றல் தோற்றம்**

ஆழக்கற்றல் தற்போது புதிய துறை போல எல்லாருக்கும் தோன்றுகிறது. ஆனால் உண்மையில் இது ஒரு பழங்கதை. மனித குலம் தோன்றிய்து போதே, மனிதன் பகுத்தறிய ஆரம்பித்தபோதே இந்த துறை தோன்றிவிட்டது எனலாம். 

ஆழக்கற்றலை பிரிந்துகொள்ள  மூளை எப்படி சிந்திக்கிறது என்பதைப் புரிந்துகொள்வது அவசியம்

1. சிந்தனை மூளையில் உதயமாகிறது
2. மூளையில் கோடிக்கணக்கான நீயூரான்கள் எனப்படும் செல்கள் உள்ளன.
3. இந்து நியூரான்கள் ஒரு ஒருங்கிணைந்த “நுண்ணறிவு” வலையமைப்பாக இணைந்து செயல்படுகின்றன, இது சிந்திக்கவும், உணரவும், செயல்படவும் உதவுகிறது

பல ஆண்டுகாளாக பல விஞ்ஞானிகள் அல்லும் பகலும் ஆராச்சிகள் செய்து மூளை எவ்வாறு செயல்படுகிறது என்பதைக் கண்டறிந்தனர்.

மூளை பழங்காலத்தில் (பி.சி.இ) இருந்து தொடர்ந்து ஆய்வு செய்யப்படுவதாக வரலாற்று சான்றுகள் காட்டுகின்றன. பண்டைய காலத்தைச் சேர்ந்த விஞ்ஞானிகள், பல நாகரிகங்களில், மூளையை அதன் செயல்பாடுகளைப் புரிந்துகொள்ள ஆய்வு செய்து, அந்த அறிவை மருத்துவ நோக்கங்களுக்காகப் பயன்படுத்தினர்.
*(குறிப்பு: சரகர், சுஷ்ருர், ஹரப்பாவில் ட்ரெபனேஷன் உள்ளிட்ட மூளையைப் படித்த இந்திய வரலாற்று அறிஞ்ஞர்களின் படங்கள் https: // bit. ly / 38tt9L5; பாபிலோனிய, எகிப்திய முயற்சிகள்).*

In the early common era (C.E) times, scientists and researchers who shared this fascination with the workings of the brain, continued efforts and by the first century, a general physical description of the brain was available to the scientific community.  Basic structures such as the soft and hard layers encasing the brain were identified in addition to a basic classification/division of the brain into functional regions (subsequently called ventricles).  

Building upon this work, over the next few centuries, physicians concluded that mental activity occurred in the brain rather than the heart, thus concurring with what some pre common era scientists had already suggested.  What seems like a small step today, was actually was a huge step forward for that time. They also concluded that the brain was the seat of the soul -- one of three "souls" found in the body, each associated with a principal organ.  Others were of the opinion that the brain was a cold, moist organ formed of sperm! 

By the Middle Ages, the anatomy of the brain was broadly believed to be consolidated around three principle divisions.  Each division localized the site of different mental activity.  Imagination was believed to be located in the anterior ventricle, memory in the posterior ventricle, and reason located in between.  Yet, there was no consensus on where sensory processing and storing of the inputs and the processing of the inputs received from the five senses (what was then called simply as “common sense”) resided.  Eleventh century scientists proposed that this “common sense” was housed in a "faculty of fantasy," that received "all the forms which are imprinted on the five senses."  Memory then preserved what common sense received.  

But by the 14th century opinions changed. It was now believed that common sense lay in the middle of the brain.  Historical records also suggest that many fundamental questions regarding the functions of the brain (including those related to common sense) remained open to debate.  Indeed, this is true to this day, but at a far more nuanced level of detail. Such differences of opinion only underscore how little was known, then, of the brain's anatomy, let alone its physiology or functioning and now we continue to learn about it to this day.

In the Renaissance period, physicians began to dissect the brain with much greater frequency, more so at the end of the fifteenth century. This, mid-sixteenth century anatomy illustration demonstrates such a dissection. Of particular note is the famous Leonardo da Vinci who also dissected and drew the brain.  Leonardo's images were considerably more anatomical in nature than a lot of his peers. He systematically examined the relationship between the brain, the olfactory and the optical nerves through experiments with wax injections to help him to model the ventricles or sections of the brain.  
He sketched the brain from many different perspectives, looking closely at the ventricles and the origins of the nerves in the medulla.  Records suggest that the more he looked, the less he became sure about the function of each section! It is now known that one of his quests was to find the location of “common sense”.  The other was to locate the seat of the soul!

Sixteenth and early seventeenth-century anatomists contributed a great deal to the physical description of the brain. Terms such as cerebrum, cerebellum and medulla were popularized. But they made few significant advances in their understanding of its functioning.  It was not until the 17th century that views on the anatomy of the brain changed significantly. Scientists began to advocate for more careful explorations of the cortex and the ventricles (two images of the brain, from the late sixteenth and mid-seventeenth centuries). The brain, finally, had a modern physiology, grounded in research and verifiable experiments. The basic concept and principles of neurology were established. The soul no longer had a home in the brain.

Then, in the late 19th century, Spanish physician Santiago Cajal (reference: figure of Santiago) formally identified neurons by staining and studying brain tissue in great detail. Cajal’s two brilliant insights — that every neuron in the brain is separate and that neurons communicate across synapses — came to be known as the neuron doctrine. Because that gap between neurons is too small to see through a light microscope, Camillo Golgi and other rigorous scientists of Cajal’s day at first dismissed the neuron doctrine as a fantasy. It would take another half-century until a new instrument, the electron microscope, could finally confirm what Cajal had seen in his mind’s eye — and carefully sketched out in thousands of stunning pen-and-ink diagrams.
 He published his observations in 1894. Subsequently, in the early parts of the 20th century, researchers began the process of understanding exactly how these neuronal cells functioned. 

   Cajal’s graceful drawings of neurons show them as separate, individual cells. He was the first to realize that the nervous system is not a network of continuous fibers, as was widely believed at the time.

Then a left shift event occurred! Computers arrived and gained momentum triggering ideas of transitioning research and experimentation from the physical world to the world of computers. Brain behavior research was considered an ideal candidate given the myriad constraints involved in acquiring specimen, performing physical experiments, deducing conclusions from results and verifying them.

Thus, in the mid 20th century, spurred by curiosity, inspired by the progresses made in the understanding of the brain and sensing the opportunity presented by their access to computing resources, Computer Scientists (a newly minted term) began experimenting with computerized simulations of the cognitive process of the brain. A subset of them chose to simulate cognitive processes by creating “artificial neurons” and then connecting them in “artificial neural networks” thus aiming to loosely mimic the physical organization and functioning of brain cells as seen in experiments involving small, localized sections of the brain.  

Warren McCulloch and Walter Pitts created a computer model using a combination of mathematics and algorithms to mimic the thought process. In their model small layers of artificial neurons passed input information to other neuronal layers until the final layer output values.  Then in his paper “The Perceptron: A Perceiving and Recognizing Automaton”, Rosenblatt shows the new avatar of McCulloch-Pitts neuron – ‘Perceptron’ that had true learning capabilities to do binary classification on it’s own. This inspires the revolution in research of shallow neural network for years to come. The Forward Propagation Artificial Neural Network had arrived!

    
Frank Rosenblatt

  
Henry J Kelley



Following this, in 1960, Henry J. Kelley improved this model by developing the basics of Continuous Back Propagation. This approach introduced the notion of feedback control into the “learning” process. Although this key concept of back propagation existed in the early 1960s, it came of practical use only around 1985. 
You might have noticed by now that all of the work in Artificial Neural Networks  that we have discussed deal with what we would today call “shallow” networks i.e. Artificial Neural Networks that just had a few connected layers between the input and the output as well as being comprised of a relatively small number of connected neurons as a whole. But Deep Learning is defined as the process of processing signals and encoding knowledge using Deep Neural Networks which are large, deeply stacked, multi-layered networks of artificial neurons comprised of millions or more networked artificial neurons.  

So, when did this shift from simple shallow networks to complex deep networks occur? 
Well, the earliest efforts in developing deep learning algorithms actually date back to 1965, when Alexey Grigoryevich Ivakhnenko and Valentin Grigorʹevich Lapa used models with polynomial (complicated equations!) activation functions, which were subsequently analyzed statistically. Then, just as things seemed to be gaining momentum, in 1970, Marvin Minsky and Seymour Papert published the book “Perceptrons” in which they show that Rosenblatt’s perceptron could not solve complicated functions like XOR. For such function perceptrons should be placed in multiple hidden layers which compromises perceptron learning algorithm. This was a setback for “bottom up” AI. 

Around the same time, there was mounting sentiment that millions had been spent on AI research with the hope that it would provide a strategic technological advantage in the cold war, but little had come out of it. There was strong criticism from the US Congress. To add to this, in 1973, leading mathematician Professor Sir James Lighthill gave a damning health report on the state of AI in the UK!

Lighthill’s view was that machines would only ever be capable of an "experienced amateur" level of chess. Common sense reasoning and supposedly simple tasks like face recognition would always be beyond their capability. Funding for the industry was slashed, ushering in what became known subsequently as the AI winter. It was a period of intense setback in the research and development of Artificial Neural Networks and Artificial Intelligence (AI) in general. Severe cutbacks in funding plagued deep learning and AI research as a whole. 

However, despite the lean times, passionate researchers carried on the work without funding! Kunihiko Fukushima developed an artificial neural network, called Neocognitron in 1979, which used a multi-layered and hierarchical design. This multi-layered, hierarchical design enabled computers to learn to recognize visual patterns. The backpropagation method was enhanced to feed errors in the output to influence and control the training of the models. This approach became widely popular when Seppo Linnainmaa wrote his master’s thesis, including FORTRAN code illustrating the backpropagation technique. The backpropagation concept was applied to neural networks and shown to work but made little impact. AI work then was primarily focused on top-down, rule based expert systems. Bottom up AI was still languishing. 

The first signs of revival were displayed when in 1985 Hinton and Williams demonstrated back propagation in a neural network which could provide interesting distribution representations. Yann LeCun followed this up by providing the first practical demonstration of backpropagation at Bell Labs in 1989 by combining convolutional neural networks with back propagation to read handwritten digits. The combination of convolutional neural networks with a backpropagation system was then used to read the numbers of handwritten checks spurring business interest. 

Interestingly, though this was a period of renewed interest, the 1985-90s actually are considered by the scientific AI community as the second lull in artificial intelligence. Hardware and software issues plagued the progress of research in neural networks and deep learning. Deep Learning algorithms while producing good results in lab conditions, struggled to scale well to industrial proportions, were quite unstable and seemed unable to generate consistent results. 
Despite these adverse conditions, passionate individuals continued to move the needle. Vladimir Vapnik and Dana Cortes developed the support vector machine which was a data driven system for mapping and recognizing similar data in 1995. Long short-term memory or LSTM was developed in 1997 by Juergen Schmidhuber and Sepp Hochreiter developed Recurrent Neural Networks (RNN).

Then the next significant deep learning advancement happened due to progress help from unexpected quarters! 
Around 1999 computers began to take advantage of GPU processing which were introduced to accelerate the massive amounts of mathematical operations needed for fast image processing and display. Faster processing meant increased computational speeds of 1000 times over a 10-year span. Deep Learning researchers were quick to note that this was exactly what deep neural networks needed and pounced upon GPUs! This quickly led to neural networks competing with support vector machines. Neural networks began to offer better results using the same data, though a tad slower when compared to a support vector machine.

Then in 2001, a research report compiled by the META Group (now called Gartner) was published outlining the challenges and opportunities of massive, three-dimensional, data growth (Volume, Velocity and Variety). This report marked the onslaught of the Big Data and Data Driven Science phenomenon. It described the increasing volume and speed of data as increasing the range of data sources and types. 

Simultaneously, on another front, the cloud phenomenon occurred giving many organizations and individuals democratized access to large compute resource. Deep Learning, which was all data driven and needed massive compute power, suddenly was back in black in a democratized fashion. Fei-Fei Li, an AI professor at Stanford (who launched ImageNet in 2009) began assembling a free, open database of more than 14 million labeled images to the community at large. These images were the inputs needed to train deep neural nets. The speed of GPUs had increased significantly by 2011, making it possible to train deep convolutional neural networks without the need for layer by layer pre-training. Deep learning now held advantages in efficiency, efficacy and speed.

 Then in 2012, Google Brain released the results of an unusual, free-spirited project called the Cat Experiment which explored the difficulties of unsupervised learning. Deep learning deployed supervised learning, which means the convolutional neural net is trained using labeled data like the images from ImageNet. This experiment used a neural net which was spread over 1,000s of computers where ten million unlabeled images were taken randomly from YouTube, as inputs to the training software.  
 
This event is considered the tipping point for Deep Learning. It made students, researchers and corporations around the world sit up, take notice and triggered a wave of scientific efforts and breakthroughs backed by corporate investment in DL. Consequently, today, DL powers several marquee apps like Amazon’s Alexa, Tesla’s Autopilot, Google’s Translation engine and many more. 

The true appeal of Deep Learning is that it has improved the accuracy of a great number of computational tasks from 95 percent to 99 percent or better— that tricky few percent that can make an automated service feel as though it works by magic. Although the concrete, interactive code examples throughout this book will dispel this apparent wizardry, deep learning has indeed imbued machines with superhuman capability on complex tasks as diverse as face recognition, text summarization, and elaborate board games. Given all these prominent advances, it is unsurprising that “deep learning” has become synonymous with “artificial intelligence” in the popular press, the workplace, and the home.

2. See bit.ly/ aiindex18 for a review of machine performance relative to humans.

In summary, these are exciting times, because, as you’ll discover over the course of this book, perhaps only once in a lifetime has a single concept has cause disruption so widely in such a short period of time. We are delighted that you too have developed an interest in deep learning and we can’t wait to share our enthusiasm for this unprecedented, transformative technique with you.


A Summary Deep Learning Timeline

1943
1943
McCulloch Pitts Neuron – Beginning
 
Walter Pitts and Warren McCulloch in their paper, “A Logical Calculus of the Ideas Immanent in Nervous Activity” shows the mathematical model of biological neuron. This McCulloch Pitts Neuron has very limited capability and has no learning mechanism. Yet it will lay the foundation for artificial neural network & deep learning.
1957
1957
Frank Rosenblatt Creates Perceptron
 
In his paper “The Perceptron: A Perceiving and Recognizing Automaton”, Rosenblatt shows the new avatar of McCulloch-Pitts neuron – ‘Perceptron’ that had true learning capabilities to do binary classification on it’s own. This inspires the revolution in research of shallow neural network for years to come, till first AI winter.
1960
1960
The First Backpropagation Model
 
Henry J. Kelley in his paper, “Gradient Theory of Optimal Flight Paths” shows the first ever version of continuous backpropagation model. His model is in context to Control Theory, yet it lays the foundation for further refinement in the model and would be used in ANN in future years.
1962
1962
Backpropagation With Chain Rule
 
Stuart Dreyfus in his paper, “The numerical solution of variational problems” shows a backpropagation model that uses simple derivative chain rule, instead of dynamic programming which earlier backpropagation models were using. This is yet another small step that strengthens the future of deep learning.
1965
1965
Birth Of Multilayer Neural Network
 
Alexey Grigoryevich Ivakhnenko along with Valentin Grigorʹevich Lapa, creates hierarchical representation of neural network that uses polynomial activation function and are trained using Group Method of Data Handling (GMDH). It is now considered as the first ever multi-layer perceptron and Ivakhnenko is often considered as father of deep learning.
1969
1969
The Fall Of Perceptron
 
Marvin Minsky and Seymour Papert publishes the book “Perceptrons” in which they show that Rosenblatt’s perceptron cannot solve complicated functions like XOR. For such function perceptrons should be placed in multiple hidden layers which compromises perceptron learning algorithm. This setback triggers the winter of neural network research.
1970
1970
Backpropagation Is Computer Coded
 
Seppo Linnainmaa publishes general method for automatic differentiation for backpropagation and also implements backpropagation in computer code. The research in backpropagation has now come very far, yet it would not be implemented in neural network till next decade.
1971
1971
Neural Network Goes Deep
 
Alexey Grigoryevich Ivakhnenko continues his research in Neural Network. He creates 8-layer Deep neural network using Group Method of Data Handling (GMDH).
1980
1980
Neocognitron – First CNN Architecture
Kunihiko Fukushima comes up with Neocognitron, the first convolutional neural network architecture which could recognize visual patterns such as handwritten characters.
1982
1982
Hopfield Network – Early RNN
 
John Hopfield creates Hopfield Network, which is nothing but a recurrent neural network. It serves as a content-addressable memory system, and would be instrumental for further RNN models of modern deep learning era.
1985
1985
Boltzmann Machine
 
Geoffrey Hinton and Terrence Sejnowski create Boltzmann Machine that is a stochastic recurrent neural network. This neural network has only input layer and hidden layer but no output layer.
1986
1986
NetTalk – ANN Learns Speech
Terry Sejnowski creates NeTalk, a neural network which learns to pronounce written English text by being shown text as input and matching phonetic transcriptions for comparison.
1986
Backpropagation In Neural Network
 
Geoffrey Hinton, Rumelhart and Williams in their paper “Learning Representations by back-propagating errors” show a new and better learning procedure for neural network by using backpropagation. It opened gates for training complex deep neural network easily which was the main obstruction in earlier days of research in this area.
1986
Restricted Boltzmann Machine
 
Paul Smolensky comes up with a variation of Boltzmann Machine where there is not intra layer connection in input and hidden layer. It is known as Restricted Boltzmann Machine (RBM). It would become popular in years to come especially for building recommender systems.
1989
1989
CNN Using Backpropagation
Yann LeCun uses backpropagation to train convolutional neural network to recognize handwritten digits. This is a breakthrough moment as it lays the foundation of modern computer vision using deep learning.
1989
Universal Approximators Theorem
 
George Cybenko publishes earliest version of the Universal Approximation Theorem in his paper “Approximation by superpositions of a sigmoidal function“. He proves that feed forward neural network with single hidden layer containing finite number of neurons can approximate any continuous function. It further adds credibility to Deep Learning.
1991
1991
Vanishing Gradient Problem Appears
 
Sepp Hochreiter identifies the problem of vanishing gradient which can make the learning of deep neural network extremely slow and almost impractical. This problem will continue to annoy deep learning community for many more years to come.
1997
1997
The Milestone Of LSTM
 
Sepp Hochreiter and Jürgen Schmidhuber publishes a milestone paper on “Long Short-Term Memory” (LSTM). It is a type of recurrent neural network architecture which will go on to revolutionize deep learning in decades to come.
2006
2006
Deep Belief Network
 
Geoffrey Hinton, Ruslan Salakhutdinov, Osindero and Teh publishes the paper “A fast learning algorithm for deep belief nets” in which they stacked multiple RBMs together in layers and called them Deep Belief Networks. The training process is much more efficient for large amount of data.
2008
2008
GPU Revolution Begins
 
Andrew NG’s group in Stanford starts advocating for the use of GPUs for training Deep Neural Networks to speed up the training time by many folds. This could bring practicality in the field of Deep Learning for training on huge volume of data efficiently.
2011
2011
Combat For Vanishing Gradient
 
Yoshua Bengio, Antoine Bordes, Xavier Glorot in their paper “Deep Sparse Rectifier Neural Networks” shows that ReLU activation function can avoid vanishing gradient problem. This means that now, apart from GPU, deep learning community has another tool to avoid issues of longer and impractical training times of deep neural network.
2012
2012
AlexNet Starts Deep Learning Boom
 
AlexNet, a GPU implemented CNN model designed by Alex Krizhevsky, wins Imagenet’s image classification contest with accuracy of 84%.  It is a huge jump over 75% accuracy that earlier models had achieved. This win triggers a new deep learning boom globally.
2014
2014
The Birth Of GANs
 
Generative Adversarial Neural Network also known as GAN is created by Ian Goodfellow. GANs open a whole new doors of application of deep learning in fashion, art, science due it’s ability to synthesize real like data.
2016
2016
AlphaGo Beats Human
 
Deepmind’s deep reinforcement learning model beats human champion in the complex game of Go. The game is much more complex than chess, so this feat captures the imagination of everyone and takes the promise of deep learning to whole new level.
2019
2019
Godfathers Win Turing Award
 
Yoshua Bengio, Geoffrey Hinton, and Yann LeCun wins Turing Award 2018 for their immense contribution in advancements in area of deep learning and artificial intelligence. This is a defining moment for those who had worked relentlessly on neural networks when entire machine learning community had moved away from it in 1970s.
 


