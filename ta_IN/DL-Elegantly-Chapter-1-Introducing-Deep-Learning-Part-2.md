
**பாடம் I: ஆழக்கற்றல் தோற்றம் (தொடர்கிறது)**

**உயிரியல் பார்வை**

**இயந்திரப் பார்வை**

உயிர்களின் ஒளி உணர் திறன் பற்றி நாம் விரிவாக விவாதித்ததற்கு காரணம் என்ன என்பது இப்போது தெளிவாகி இருக்கும் என்று எண்ணுகிறேன். இன்றைய ஆழக்கற்றலுக்கும் உயிர்களின் ஒளி உணர் திறனுக்கும் உள்ள தொடர்பை நான் இப்போது நான் நன்றாத தெரிந்து கொண்டோம்.


*இந்த படத்தில் உயிர்களின் ஒளி உணரும் திறனில் ஏற்பட்ட மாற்றங்களையும் இயந்திரக்கற்றல் வளர்சியையும் காட்டுகிறது.  நீல நிறக்கோடு ட்ரைலோபைட்டுகளில் பார்வை வளர்ச்சியையும், ஹூபல் மற்றும் வீசலின் 1959 இல் முதன்மைக் கார்ட்டெக்ஸ் ஆய்வையும் காட்டுகிறது. இயந்திரப் பார்வை(machine vision) கால வரிசை பிங்க், மற்றும் ஊதா நிற கோடுகளாகப் பிரிக்கப்பட்டுள்ளது. பிங்க் நிற கோடு ஆழக்கற்றல் வளர்சியையும் ஊதா நிற கோடு இயந்திரக்கற்றல் வளர்சியையும் காட்டுகிறது.*


**நியோகாக்னிட்ரானை அறிவோம்**

முதன்மை காட்சி புறணி படிநிலையை(hierarchy) உருவாக்கும் எளிய மற்றும் சிக்கலான செல்களை ஹூபல் மற்றும் வைசல் கண்டுபிடித்தனர்.   ஹூபல் மற்றும் வைசல் கண்டுபிடிப்பால்   ஈர்க்கப்பட்டு, 1970 களின் பிற்பகுதியில், ஜப்பானிய மின் பொறியாளர் குனிஹிகோ புகுஷிமா இயந்திர பார்வைக்கு ஒரு ஒத்த கட்டமைப்பை முன்மொழிந்தார். அதற்கு அவர் நியோகாக்னிட்ரான் என்று பெயரிட்டார்.  புகுஷிமா தனது எழுத்தில் ஹூபல் மற்றும் வீசலின்  முதன்மைக் காட்சிப் புறணி அமைப்பைப் பற்றிய அவர்களின் மூன்று முக்கிய கட்டுரைகளை மேற்கோள்காட்டி விளக்கினார்.



*குனிஹிகோ புகுஷிமா(Kunihiko Fukushima)*

*Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cynbernetics, 36, 193– 202.* 

*படிநிலை முறையில் செயற்கை நியூரான்களை ஒழுங்குபடுத்துவதன் மூலம், செயற்கை நியூரான்கள் - உயிர்ப்புள்ள உண்மை நியூரான்கள் போலவே, - *
*By arranging artificial neurons in this hierarchical manner, these artificial neurons— like their biological inspiration earlier — generally represent line orientations in the cells of the layers closest to the raw visual image, while successively deeper layers represent successively complex, successively abstract objects.*



**LeNet-5**
நியோ காக்நெட்ரானை கையெழுத்த


நியோகாக்னிட்ரான் கையால் எழுதப்பட்ட எழுத்துக்களை அடையாளம் காணும் திறன் கொண்டதாக இருந்தாலும்,யான் லீகூன் மற்றும் யோஷுவா பென்ஜியோ இருவரும் இணைந்து இன்னும் துள்ளியமான மற்றும் அதிக செயல் திறன் கொண்ட LeNet-5 ஐ உருவாக்கினார்கள். புகுஷிமாவின் தலைமையில்  ஹூபல் மற்றும் வைசலின் ஆராய்சியை மையமாகக் கொண்டு, லெனெட் -5 உருவாக்கப்பட்டது.  யான் லீகூன் மற்றும் யோஷுவா பென்ஜியோ தரமான தரவுகள், அதிக திறன் கொண்ட கணினிகள் மற்றும் பின்னோக்கு பரவல் அல்காரிதம்களைக் கொண்டு அவர்கள் லெனெட் -5 ஐ உருவாக்கினார்கள்.


செயற்கை நரம்பியல் வலையமைப்பு மற்றும் ஆழக்கற்றல் ஆராய்ச்சியில்  பாரிஸில் பான் - யான் மற்றும் லீகன் இருவரும் மிக முக்கியமானவர்கள். லீகன் நியூயார்க் பல்கலைக்கழக அறிவியல் மையத்தின் நிறுவன இயக்குநராகவும், பேஸ்புக் நிறுவனத்தில் AI ஆராய்ச்சியின் இயக்குநராகவும் உள்ளார்.


யோஷுவா பெங்கியோ செயற்கை நரம்பியல் நெட்வொர்க்குகள் மற்றும் ஆழ்ந்த கற்றல் துறையில் மிகவும் முக்கியம் வாந்த விஞ்ஞானி. பிரான்சில் பிறந்த இவர், மாண்ட்ரீல் பல்கலைக்கழகத்தில் கணினி அறிவியல் பேராசிரியராக உள்ளார், மேலும் கனடிய இன்ஸ்டிடியூட் ஃபார் அட்வான்ஸ்ட் ரிசர்ச்சில் புகழ்பெற்ற இயந்திரங்கள் மற்றும் மூளை திட்டத்தின் இணை இயக்குனராக உள்ளார்.



மேலே உள்ள இரண்டு வரைபடங்கள் ஒரே லெனெட் -5 கட்டமைப்பின் இரண்டு விதமாக காட்டப்பட்டுள்ளது, முதலாவது ஒரு தர்க்கரீதியான பார்வை மற்றும் இரண்டாவது ஒரு “தொழில்நுட்ப” பார்வை. ஹூபல் மற்றும் வீசல் ஆகியோரால் கண்டுபிடிக்கப்பட்ட முதன்மை காட்சி கோர்டெக்ஸின் படிநிலை கட்டமைப்பை லெனெட் -5 எவ்வாறு தக்க வைத்துக் கொள்கிறது என்பதையும்பு குஷிமா தனது நியோகாக்னிட்ரானில் எவ்வாறு பயன்படுத்தியுள்ளார் என்பதையும் காணலாம். படத்தில் இடதுபுற அடுக்கு எளிய விளிம்புகளையும், அடுத்தடுத்த அடுக்குகள் பெருகிய முறையில் சிக்கலான அம்சங்களையும் குறிக்கின்றன. இந்த வழியில் தகவல்களைச் செயலாக்குவதன் மூலம், கையால் எழுதப்பட்ட “2”, எடுத்துக்காட்டாக, எண் இரண்டாக சரியாக கண்டறியும்.

மேலே உள்ள படத்தில் லெனெட்-5 எவ்வாறு செயல்படுகிறது என்று பார்க்கலாம். அது எண் இரண்டை (2) உள்வாங்கி அடுத்த அடுக்குக்கு அனுப்புகிறது. இந்த அடுக்கில் உள்ளிடப்பட்ட தகவலின் அம்சங்கள்(features) பெரிய அம்சங்களாக பிரிகிறது. அடுத்த அடுக்கில் சிறு சிறு அம்சங்களாக பிரிகிறது. அடுத்த அடுக்கில் சிறு கன சதுரங்களாக காட்டப்பட்டுள்ளது. அடுத்தடுத்த அடுக்குகளில் இந்த கன சதுரங்கள் தடிமன் குறைந்து நீளம் அதிகரித்து காணப்படும். இறுதியாக கோடுகள் போல காட்சியளிக்கும்.  இறுதி அடுக்கு சரியான எண்ணை விடையாக வெளியிடும்.

*Fukushima, K., & Wake, N. (1991). Handwritten alphanumeric character recognition by the neocognitron. IEEE Transactions on Neural Networks, 2, 355– 65.*

*LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 2, 355– 65. 11.*

லெனெட்-5 என்ற மாற்றத்த நரம்பியல் வலைப்பின்னல் ஒரு ஆழக்கற்றல் வகையைச் சேர்ந்த அல்காரிதம் ஆகும். இந்த வலையமைப்பு பிற்காலத்தில் இயந்திரப்பார்வை எனப்படும் மிஷன் விஷன் திட்டத்தில் முக்கிய பங்கு வகிக்க்கவிருக்கிறது. ெனெட்-5இல் பயன்படுத்திய  MNIST எனப்படும் கையாள் எழுதப்பட்ட எண்களின் மாதிரி தரவுகளை  புத்தகத்தின் பகுதி -2 இல் "அத்தியாவசிய கோட்பாடுகள் மற்ற்றும் விளக்கங்கள்" என்ற பகுதியில் அதிகம் பயன்படுத்தவிருக்கிறோம். ஆழக்கற்றல் மாதிரிகளில் அதிகம் பயன்படுத்தப்படும் பின்னோக்கு பரவல் முறையை பின் வரும் பகுதிகளில் தெரிந்து கொள்ளலாம்.

இதனைத் தொடர்ந்து அமெரிக்க தபால் சேவை நிறுவனம் பின்கோடுகளைக் கண்டுபிடிக்க லெனெட்-5ஐ வணிகரீதியாக பயன்படுத்தத் தொடங்கியது. எனவே லெனெட் 5 மூலம் சாதாரண இயந்திரக் கற்றலுக்கும் ஆழக்கற்றலுக்கும் உள்ள வேறுபாட்டை தெளிவாக தெரிந்துகொள்ளலாம்.

படம் 1.12 இல் உள்ளது போல் இயந்திரக்கற்றல் முறை முக்கிய அம்சங்களைக் கற்றல் முறையைப் பயன்படுத்துகிறது. ஒரு தரவை எடுத்து அதை தூய்மையாக்கி, முக்கிய அம்சங்களைத் தேர்ந்தெடுத்து புள்ளி விவரங்களின் அடிப்படையில் மாதிரிகளை உருவாக்கலாம்.  நிக் ரிக்ரஷன், ரேண்டம் ஃப்பாரஸ்ட் மற்ற்றும் வெக்டார் இய்ந்திரம் போன்ற முறைகள்  பதப்படுத்தப்படாத தரவுகளைக் கொண்டு சிறப்பான முடிவுகளைத் தருகிறது.


இயந்திரக் கற்றலில் அம்சங்களை தேர்ந்ந்நதெடுத்தல் என்பது ஒரு அறிவியல் மற்றும் கலை எனலாம். இதற்கு துறையில் நிபுணத்துவமும், உள்ளுணர்வும், கணிதத்திறமையும் மிகவும் முக்கியம். மிகச்சிறந்த தரவு முடிகள் பெரும்பாலும் மிகக் குறைந்த அளவே கணக்கீடுகளை பயன்படுத்தியுள்ளது.  பொதுவாக வளமான தரவுகளும் அந்த தரவுகளை பயன்படுத்தும் அல்காரிதம்கள் மற்றும்  தரவுடன் தொடர்புடைய கட்டமைப்புகள்/பண்புகள் போன்றவைகளே சிறந்த முடிவுகளைத்தரும்.  அம்சங்களை தேர்தெடுக்கும் போது தரவுகளின் பண்புக்கூறுகளை(Attributes)  தரவுகளின் அம்சங்களாக மாற்றவேண்டும்.  தரவுகளின் பண்புக்கூறுகள் என்பது தரவுகளின் பரிணாமம் ஆகும்.  தரவின் அம்சங்களைத் தேர்வு செய்யும் போது, தரவுகளை அலசி ஒழுங்கு படுத்துவதன் மூலம் மாதிரி/கற்றல் அல்காரிதம்களை குறைந்த இரைச்சலுடன் திறம்பட செயல்பட வைக்க முடியும். இரைச்சல் என்பது இங்கே சம்மந்தம் இல்லாத தரவுகள் என எடுத்துக்கொள்ளலாம்.


இயந்திரக்கற்றல் முறையில் பதபடுத்தபடாத தரவுகளின்(raw data) அம்சங்களை ஆராய்ந்து தேவைக்கு ஏற்ப தரவுகளை(data) மாற்றவேண்டும். ஆனால் இயந்திரக்கற்றல்(Machine Learning) முறையில் அம்சங்களை ஆராய தேவை இல்லை. ஆனால் இயந்திரக்கற்றல் மாதிரி கட்டமைப்பை உருவாக்குவதற்கு அதிக நேரம் தேவைப்படுகிறது. இயந்திரக்கற்றல் முறையில் மாதிரிகளை உருவாக்குவதும் அதில் சிறந்த மாதிரியை தேர்வு செய்து பயன்படுத்துவதற்கும் அதிக நேரம் செலவாகும்.


இருப்பினும்  அம்சங்களை தேர்வெடுப்பதில் இத்தனை சிக்கல் இருந்தாலும் இயந்திரக்கற்றலில் அதிக நம்பகத்தன்மை உண்டு என்று நிறுவனங்கள் நம்பியதால் ஆழக்கற்றக்கான நிதி சரிவர கிடைக்காமல் போனது. மேலும் மோசமான நிறையை(weight) யுடன் துவக்கம், கோவேரியன் நகர்வு, பயனற்ற சிக்மாய்ட் செயல்பாடு. என அனைத்தும் சேர்ந்து ஆழக்கற்றலை சற்று பின்னுக்குத் தள்ளியது.  ஆனால் கனடா அரசு மான்றியல் பல்கலைக் கழகதின் மூலம் செயற்கை நுண்ணறிவுக்கு நிதி ஒதுக்கியது. எனவே மான்றியல் பல்கலைக்கழகம் ஆழக்கற்றல் துறையில் சிறந்து விளங்குகிறது.

*2000 களின் முற்பகுதியில் பால் வயோலா மற்றும் மைக்கேல் ஜோன்ஸ் ஆகியோரிடமிருந்து வெற்றிகரமான அம்சங்களை தேர்வு செய்வதில் புகழ்பெற்ற உதாரணத்தை இங்கே காணலாம். வயோலா மற்றும் ஜோன்ஸ் படத்தில் காட்டப்பட்டுள்ள செங்குத்து அல்லது கிடைமட்ட கருப்பு மற்றும் வெள்ளை பார்கள் போன்ற செவ்வக வடிப்பான்களைப் பயன்படுத்தினர். ஒரு படத்தின் மீது செவ்வக வடிப்பான்களைக் கடந்து செல்வதன் மூலம் உருவாக்கப்படும் இந்த அம்சங்கள் ஒரு முகத்தின் இருப்பை நம்பத்தகுந்த முறையில் கண்டறிய இயந்திர கற்றல் வழிமுறைகளில் வழங்கப்பட்டன. அவற்றின் திறமையான வழிமுறை புஜிஃபில்ம் கேமராக்களில் நுழைந்து, நிகழ்நேர ஆட்டோ-ஃபோகஸை எளிதாக்கியது. படத்தில், செவ்வக வடிப்பான்களின் சேர்க்கைகள்  நம்பகமான முகத்தை அடையாளம் காணும் நோக்கத்திற்காக பயன்படுத்தப்பட்டுள்ளது. முதல் வடிகட்டி கிடைமட்ட கருப்பு செவ்வக பெட்டியின் உள்ளே கிடைமட்ட வெள்ளை பட்டை. இரண்டாவது வடிகட்டி ஒரு கருப்பு செவ்வக பெட்டியின் உள்ளே ஒரு வெள்ளை செங்குத்து பட்டை. அசல் படம் ஒரு நபரின் முகத்தின் சிறு உருவமாகும். இரண்டு வடிப்பான்களும் நபரின் முகத்தின் கண் பகுதியை உள்ளடக்கும். இதுவே உலகில் முதல் முதலாக இயந்திரம் மூலம் மனிதனிம் முகம் அடையாளம் காணப்பட்ட தருணமாகும்*


மனித முகத்தின் பண்புகளை பல வருடங்களாக ஆராய்ந்து பிக்ஸெல்களை வடிகட்டி முகத்தை கண்டறிவதற்கான இயந்திரக்கற்றல் மாதிரியை வடிவமைத்தார்கள். இவ்வளவு கடின உழைப்பால் உருவாக்கப்பட்ட மாதிரிகள் மனித முகங்களைக் கண்டுபிடித்தன ஆனால் இன்னாருடைய முகம் என்று அடையாளம் காட்ட முடியவில்லை.  ஒரு குறிப்பிட்ட நபரின் முகத்தை துள்ளியமாக அறியவும், அல்லது முகம் அல்லாத பொருள்களை உதாரணமாக இது வீடு, இது கார், இது பந்து என வகைப்படுத்தி அறியவும் இன்னும் பலகாலம் ஆகலாம். மிஷன் விஷன் சமூகம் இதற்காக உழைத்துக்கொண்டுகிறது.


**இமேஜ் நெட் மற்றும் ILSVRC**

லெனெட்-5 இன் வெறிக்க்கு காரணியாய் அமைந்தது தரமான அதிக அளவிலான தரவுகளே.  பல ஆய்வுகளின் முடிவுகள் தரவுகளை அடிப்படையாகக் கொண்டுள்ளதை அறிவியல் சமூகமும் முழுமையாக அங்கீகரித்தது.
ஃபீ-ஃபை லி 22000 வகைக்கொண்ட 14 மில்லியன் படங்களைக்கொண்ட மிகப்பெரிய இமேஜ்நெட் தரவுத்தொகுப்பை(dataset) வெளியிட்டார். இந்த தரவுதொகுப்பில் கப்பல்கள், சிறுத்தைகள்,
நட்சத்திர மீன், மற்றும் பழங்கள் என அத்தனை வகையான தரவுகளும் இருந்தது.  இந்த தரவு  நரம்பியல் வலைப்பின்னலின் வளர்சிக்கு ஒரு திருப்புமுனையாக அமைந்தது.


**ஃபீ-ஃபை லி**
ப்ரின்ஸ்டன் பல்கலைக்கழதின் பேராசியராக இருந்த ஃபீ-ஃபை லி மற்றும் அவருடன் பணியாற்றிய சிலருடன் சே இமேஜ்நெட்டை உருவாக்கின்னார். இப்போது அவர் 
ஸ்டான்போர்ட் பல்கலைக்கழகத்தில் ஆசிரியராகவும் கூகிள் நிறுவனத்தில் A.I./ ML தலைமை விஞ்ஞானியாகவும் பணியாற்றுகிறார். 2010 ஆம் ஆன்இல் அவர் ILSVRC(the ImageNet Large Scale Visual Recognition Challenge) எனப்படும் இமேஜ்நெட் தரவுத்தொகுப்பில் இருந்து ஒருபகுதி(1.4 மில்லியன் படங்கள் மற்றும் 1000 வகைகள்) தரவைக்கொண்டு மாதிரிகளை உருவாக்கும் போட்டிகளை நடத்தினார்கள். இந்ததரவில் நாய்களும் நாய்களின் இனமும் வகைப்படுத்தப்பட்டிருந்தது.  இதனால் நாயைக் கண்டறிவதோடு நிற்காமல் அது என்ன வகை என்பதையும் அறிய முடிந்தது.



உதாரணமாக  ஒரு பரிசோதனையாக, ராம்பூர் கிரேஹவுண்டின் புகைப்படங்களை முடோல் ஹவுண்டிலிருந்து வேறுபடுத்த முயற்சிக்கவும். இது கடினமானது, ஆனால் நாய் வல்லுநர்கள் மற்றும் ஆழக் கற்றல் இயந்திர பார்வை மாதிரிகள் இதை எளிதாக செய்யலாம்!

**ராம்பூர் கிரேஹவுண்டின் vs முடோல் ஹவுண்டிலிருந்து**



The arrival of the gamechanger a.k.a. AlexNet
**அலெக்ஸ்நெட்டின் வருகை**

ஐ.எல்.எஸ்.வி.ஆர்.சியின் முதல் இரண்டு ஆண்டுகளில் போட்டியில் நுழைந்த அனைத்து மாதிரிகளும்  பாரம்பரிய இயந்திர கற்றல் சித்தாந்தத்தில் உருவானவை என்பதை மேலே உள்ள வரைபடம் காட்டுகிறது. 2012 ஆம் ஆண்டு ஜெஃப்ரி ஹிண்டன் தலைமையிலான டொராண்டோ பல்கலைக்கழக ஆய்வகத்தில் பணியாற்றும் அலெக்ஸ் கிரிஷெவ்ஸ்கி மற்றும் இலியா சுட்ஸ்கெவர், தற்போதுள்ள அனைத்து மாதிரிகளையும் விஞ்சக்கூடிய மிகச்சிறந்த மாதிரியை சமர்ப்பித்தனர். அதை அலெக்ஸ்நெட் என்று அழைத்தனர்.

அலெக்ஸ்நெட்டின் இந்த வெற்றி ஆழக்கற்றல் துறையில் மிக முக்கிய மாற்றங்களைக் கொண்டுவந்தது. 2012 க்கு பிறகு சமர்பிக்கப்பாட பெரும்பாலான மாதிரிகள் ஆழக்கற்றலைப் பயன்படுத்தின. பல நிறுவனங்கள் அவரவர் தரவுகளில் ஆழக்கற்றலை பயன்படுத்த ஆரம்பித்தனர்.

2015 இல் இயந்திரங்கள் மனிதனைவிட மிகவும் துள்ளியமான முடிவுகளை தர ஆரம்பித்தது. 

*வரைபடத்தில், கிடைமட்ட அச்சு 2010 மற்றும் 2017 வரையிலான ஆண்டுகள் மற்றும் வரம்புகளைக் குறிக்கிறது. செங்குத்து அச்சு முதல் 5 பிழை வீதத்தைக் குறிக்கிறது 0 முதல் 0.6 வரை இருக்கும். செங்குத்து அச்சு 0 முதல் 0.6 வரை இருக்கும் முதல் 5 பிழை வீதத்தைக் குறிக்கிறது. 2010 ஆம் ஆண்டிற்கான ஒரு பெட்டி வடைபடம் காட்டப்பட்டுள்ளது, இதில் குறைந்தபட்ச பிழை மதிப்பு 0.26 ஆகவும் அதிகபட்ச மதிப்பு 0.45 ஆகவும் குறைகிறது. 2011 ஆம் ஆண்டில், பெட்டி  வரைபடம் அதிகபட்ச மதிப்பு 0.5 மற்றும் குறைந்தபட்ச மதிப்பு 0.24 ஆகியவற்றைக் காட்டுகிறது. 2012 ஆம் ஆண்டில், பாரம்பரிய இயந்திர கற்றல் 0.23 மதிப்பைக் காட்டுகிறது, ஆனால் அலெக்ஸ்நெட் 0.18 என்ற குறைந்த பிழை விகிதத்தைக் காட்டியது. 2012 வரை பாரம்பரிய இயந்திர கற்றல், அலெக்ஸ்நெட் அறிமுகப்படுத்தப்பட்ட பின்னர், அடுத்தடுத்த ஆண்டுகள் ஆழ்ந்த கற்றலின் செயல்திறனைக் காட்டுகிறது. பிழை விகிதங்கள் 2017 ஆம் ஆண்டு வரை குறைந்து வரும் போக்கைக் கொண்டுள்ளன.*

பிரபலமான பத்திரிகைகளில் "ஆழக் கற்றலின் பிதா" ிரிட்டிஷ்-கனடிய செயற்கை நரம்பியல் வலையமைப்பு முன்னோடி எஃப்ஃப்ரே ஹிண்டன் ஆவார். அவர் டொராண்டோ பல்கலைக்கழகத்தில் ஓய்வுபெற்ற பேராசிரியராகவும், கூகிளில் நிறுவனத்தில் கூளில் ப்ரெயி அணியை நிர்வகிக்கும் பொறுப்பில் உள்ளார். ஆழ்ந்த கற்றல் தொடர்பான பணிகளுக்காக, 2019 ஆம் ஆண்டில், ஹிண்டன், யான் லீகன் மற்றும் யோஷுவா பெங்கியோ ஆகியீர் டூரிங் விருது-கணினி அறிவியலில் மிக உயர்ந்த  கரவ விருதைப் பெற்றனர்.

You can see in this figure that AlexNet’s hierarchical architecture is reminiscent of LeNet-5, with the first (left-hand) layer representing simple visual features like edges, and deeper layers representing increasingly complex features and abstract concepts. Shown at the bottom are examples of images to which the neurons in that layer maximally respond, recalling the layers of the biological visual system and demonstrating the hierarchical increase in visual feature complexity. In the example shown here, an image of a cat input into LeNet-5 is correctly identified as such (as implied by the green “CAT” output). “CONV” indicates the use of something called a convolutional layer, and “FC” is a fully connected layer (don’t worry we will formally explain these layer types in later chapters).

Though the architecture of AlexNet is reminiscent of LeNet-5, there are three principal factors that enabled AlexNet to be the state-of-the-art machine vision algorithm in 2012:

Training data: Not only did Krizhevsky and his colleagues have access to the massive ImageNet index, they also artificially expanded the data available to them by applying transformations to the training images (you, too, will do this later).

Processing power: Not only had computing power per unit of cost increased dramatically from 1998 to 2012, but Krizhevsky, Hinton, and Sutskever also programmed two GPUs24 to train their large datasets with previously unseen efficiency.

Architectural Advances: AlexNet is deeper (has more layers) than LeNet-5, and it takes advantage of both a new type of artificial neuron and a nifty trick that helps generalize deep learning models beyond the data they’re trained on.

GPU stands for "Graphics Processing Unit." A GPU is a processor designed to handle graphics operations. This includes both 2D and 3D calculations, though GPUs primarily excel at rendering 3D graphics. Early PCs did not include GPUs, which meant the CPU had to handle all standard calculations and graphics operations. As software demands increased and graphics became more important (especially in video games), a need arose for a separate processor to render graphics. On August 31, 1999, NVIDIA introduced the first commercially available GPU for a desktop computer, called the GeForce 256.

It could process 10 million polygons per second, allowing it to offload a significant amount of graphics processing from the CPU. The success of the first graphics processing unit caused both hardware and software developers alike to quickly adopt GPU support. Motherboards were manufactured with faster PCI slots and AGP slots, designed exclusively for graphics cards, became a common option as well. Software APIs like OpenGL and Direct3D were created to help developers make use of GPUs in their programs. Today, dedicated graphics processing is standard – not just in desktop PCs – but also in laptops, smartphones, and video game consoles. These are designed primarily for rendering video games but are well suited to performing the matrix multiplication that abounds in deep learning across hundreds of parallel computing threads.

What really makes the industry practitioners view Deep Learning as “Disruptive”?

The ILSVRC case study underlines why deep learning models like AlexNet are considered widely applicable and disruptive across industries and computational applications. These models dramatically reduce the subject-matter expertise required for building highly accurate predictive models. This trend away from expertise-driven feature engineering and toward surprisingly powerful automatic-feature-generating deep learning models has been prevalently borne out across not only vision applications, but also, for example, the playing of complex games (the topic of Chapter 4) and natural language processing (Chapter 2).

Today, one no longer needs to be a specialist in the visual attributes of faces to create a face-recognition algorithm. One no longer requires a thorough understanding of a game’s strategies to write a program that can master it. One no longer needs to be an authority on the structure and semantics of each of several languages to develop a language-translation tool. For a rapidly growing list of use cases, one’s ability to apply deep learning techniques outweighs the value of domain-specific proficiency. While such proficiency formerly may have necessitated a doctoral degree or perhaps years of postdoctoral research within a given domain, a functional level of deep learning capability can be developed with relative ease— as by working through this book!

TensorFlow Playground

For a fun, interactive way to crystallize the hierarchical, feature-learning nature of deep learning, make your way to the TensorFlow Playground at bit.ly/ TFplayground. When you use this custom link, your network should automatically look similar to the one shown in the figure below.

In Part II we will return to define all of the terms on the screen; for the present exercise, they can be safely ignored. It suffices at this time to know that this is a deep learning model. The model architecture consists of six layers of artificial neurons: an input layer on the left (below the “FEATURES” heading), four “HIDDEN LAYERS” (which bear the responsibility of learning), and an “OUTPUT” layer (the grid on the far right ranging from –6 to + 6 on both axes).

The network’s goal is to learn how to distinguish orange dots (negative cases) from blue dots (positive cases) based solely on their location on the grid. As such, in the input layer, we are only feeding in two pieces of information about each dot: its horizontal position (X1) and its vertical position (X2). The dots that will be used as training data are shown by default on the grid. By clicking the Show test data toggle, you can also see the location of dots that will be used to assess the performance of the network as it learns. Critically, these test data are not available to the network while it’s learning, so they help us ensure that the network generalizes well to new, unseen data.

This deep neural network is ready to learn how to distinguish a spiral of orange dots (negative cases) from blue dots (positive cases) based on their position on the X1 and X2 axes of the grid on the right. "A screenshot shows the Tensorflow interface. The interface is divided into four parts, vertically: data, features, hidden layers, and output. Data: four types of dataset are given, of which the spiral type is selected. The ratio of training to test data is set to 50 percent, the noise is set to 0 and the batch size is set to 10. The section has "Regenerate" command button. The next one is "Features" which includes options to select several input layers. Here, two inputs X1 and X2 are selected. To the right of the 'features' is the hidden layers section, with 4 layers placed one next to another. There are add and delete buttons to increase or reduce the number of neurons in each layer. The 4 layers are interconnected. Here, the first layer has 8 neurons, the second layer has 8 neurons, the third layer has 4 neurons and the fourth layer has 2 neurons. There are add and delete buttons to modify the number of hidden layers too. The final hidden layer is connected to the output.

The output here shows a spiral-shaped graph constituted of orange and blue dots lying alternatively in the graph. In the output, test loss is 0.501 and training loss is 0.509. Below the output is a scale where "colors shows data, neurons and weight values." For pure orange it starts from -1, for white, it is 0, and for blue it is 1. Below this are two checkboxes titled "Show test data" and "Discretize output," both not selected. Click the prominent Play arrow in the top-left corner. Enable the network to train until the “Training loss” and “Test loss” in the top-right corner have both approached zero— say, less than 0.05. How long this takes will depend on the hardware you’re using but hopefully will not be more than a few minutes.

As captured in the figure below, you should now see the network’s artificial neurons representing the input data, with increasing complexity and abstraction the deeper (further to the right) they are positioned— as in the neocognitron, LeNet-5, and AlexNet. Every time the network is run, the neuron-level details of how the network solves the spiral classification problem are unique, but the general approach remains the same (to see this for yourself, you can refresh the page and retrain the network).

The artificial neurons in the leftmost hidden layer are specialized in distinguishing edges (straight lines), each at a particular orientation. Neurons from the first hidden layer pass information to neurons in the second hidden layer, each of which recombines the edges into slightly more complex features like curves. The neurons in each successive layer recombine information from the neurons of the preceding layer, gradually increasing the complexity and abstraction of the features the neurons can represent. By the final (rightmost) layer, the neurons are adept at representing the intricacies of the spiral shape, enabling the network to accurately predict whether a dot is orange (a negative case) or blue (a positive case) based on its position (its X1 and X2 coordinates) in the grid. Hover over a neuron to project it onto the far-right “OUTPUT” grid and examine its individual specialization in detail.

Figure 1.19 The network after training The TensorFlow screen shows the output of the trained network. The spiral dataset is selected. The input layer has values X1 and X2 fed into it. There are 4 hidden layers, that have 8 neurons, 8 neurons, 4 neurons, and 2 neurons respectively. The number of hidden layers and the number of neurons in a layer are editable. The output shows a test loss of 0.010 and a training loss of 0.004. The final output shows the blue and orange spiral surrounded by blue or orange region. The blue spiral lies within the blue region and the orange spiral lies within the orange region. This shows that the trained network correctly detected the blue and orange dots with the X1 and X2 coordinates.

Realtime doodling and machine vision

To interactively experience a deep learning network carrying out a machine vision task in real time, navigate to quickdraw.withgoogle.com to play the Quick, Draw! game.

Click Let’s Draw! to begin playing the game. You will be prompted to draw an object, and a deep learning algorithm will guess what you sketch. By the end of Chapter 10, we will have covered all of the theory and practical code examples needed to devise a machine vision algorithm similar to this one! To boot, the drawings you create will be added to the dataset that you’ll leverage in Chapter 12 when you create a deep learning model that can convincingly mimic human-drawn doodles. So, come with us on this fantastic journey!

Summary In this chapter, we explored the history of deep learning from its evolutionary origins through to the AlexNet watershed moment in 2012 that brought the technique to the forefront of scientific attention. Through all of this, the point to keep in mind is that it is the hierarchical architecture of deep learning models that enables them to encode increasingly complex representations. We explored this concept with an interactive demonstration of hierarchical representations in action by training an artificial neural network in the TensorFlow Playground. We then looked at a system that recognizes doodles “automagically” using deep learning!

In the next chapter, we will expand on the ideas introduced in this chapter by moving from vision applications to language applications!
<br/>
<br/>
